# Model Architecture Configuration
# n_layer: 12           # Number of transformer layers
# n_heads: 12           # Number of attention heads
# n_embd: 768           # Embedding dimension
# vocab_size: 50304   # Vocabulary size (GPT-2 default)
dropout: 0.1          # Dropout probability
activation: "gelu"    # Activation function: "gelu" or "relu"

# Context/Sequence Configuration
# context_len: 1024     # Maximum sequence length
# seq_len: 1024 
# Training Configuration
seed: 42              # Random seed for reproducibility
epochs: 5            # Number of training epochs
batch_size: 2       # Batch size for each gradient step
mega_batch_size: 512  # Size of batches loaded from DataLoader
grad_clip: 1.0        # Gradient clipping
# Optimizer Configuration
optimizer: "adamw"    # Optimizer: "adamw" or "adam"y
max_lr: 6.0e-4        # Maximum learning rate
min_lr: 6.0e-5        # Minimum learning rate
beta1: 0.9            # Beta1 for Adam/AdamW
beta2: 0.95           # Beta2 for Adam/AdamW
weight_decay: 0.1     # Weight decay
warmup_steps: 720    # Number of warmup steps
# Checkpoint Configuration
checkpoint_dir: "checkpoints"  # Directory to save checkpoints
save_every: 1         # Save checkpoint every N epochs

# Logging Configuration
   
log_model_summary: true  
generate_interval: 5000
sample_max_tokens: 100  


cache_dir: "./data"

dataset:
  cache_dir: "./data"
  target_tokens: 20_000_000_000  
  subsets:
    - name: "Fineweb-EDU"
      hf_path: "HuggingFaceFW/fineweb-edu"
      split: "train"
      max_tokens: 5_500_000_000  
      text_field : "text" 
      context_len: 1024
      kwargs:
        name: "sample-10BT"
    - name: "Stack"
      hf_path: "bigcode/the-stack"
      split: "train"
      max_tokens: 400_000_000 
      text_field : "content" 
      context_len: 1024
      kwargs:
        data_dir: "data/python"
    - name: "OpenWebText"
      hf_path: "Skylion007/openwebtext"
      split: "train"
      max_tokens: 2_000_000_000 
      text_field : "text" 
      context_len: 1024


n_layer: 4           # Number of transformer layers
n_heads: 4          # Number of attention heads
n_embd: 256           # Embedding dimension
vocab_size: 2048   # Vocabulary size (GPT-2 default)
dropout: 0.1          # Dropout probability
activation: "gelu"    # Activation function: "gelu" or "relu"

# Context/Sequence Configuration
seq_len: 64
wandb:
  project_name: "gpt-2-small-cc-stories"
  entity: "training-transformers-vast"
  log_interval: 1000
  log_gradients: true